---
title: "Approximation Benchmark"
output:
  rmarkdown::html_vignette:
    toc: true
    fig_caption: true
    self_contained: yes
fontsize: 11pt
documentclass: article
bibliography: references.bib
csl: reference-style.csl
vignette: >
  %\VignetteIndexEntry{Approximation Benchmark}
  %\VignetteEngine{knitr::rmarkdown_notangle}
---

```{r, include = FALSE}
h <- 3.5
w <- 3.5
is_check <- ("CheckExEnv" %in% search()) || any(c("_R_CHECK_TIMINGS_",
             "_R_CHECK_LICENSE_") %in% names(Sys.getenv()))
knitr::opts_chunk$set(fig.align = "center", eval = !is_check)
```

```{r, include = FALSE}
devtools::load_all()
```

Here we will verify that the approximation methods are valid for value of information analyses. To achieve this, we will compare estimates from the approximation methods against the correct values from the exact methods. We will also examine how the number of approximation states affects the quality of the estimate.

## Setup

Let's start by setting up our R session. Here we will load some R packages and pre-set the random number generators for reproducibility.

```{r, message = FALSE, warning = FALSE}
# load packages
library(surveyvoi)
library(dplyr)
library(tidyr)
library(ggplot2)
library(tibble)
library(gridExtra)
library(viridis)
library(RandomFields)

# set RNG seeds for reproducibility
set.seed(505)
RFoptions(seed = 505)

# set default table printing options
options(pillar.sigfig = 6)
```

## Simulate data

Let's simulate some data. To keep things simple, we will simulate data for 10 sites and 1 conservation feature (e.g. species). Of the 10 sites in total, we will simulate survey data for 5 sites---meaning that 10 sites will not have survey data.

```{r}
# simulate site data
site_data <- simulate_site_data(
  n_sites = 10, n_features = 1, proportion_of_sites_missing_data = 5 / 10)

# print site data
print(site_data)

# plot the spatial location of the sites
ggplot(site_data) +
  geom_sf() +
  ggtitle("sites")
```

The `site_data` object is a spatially explicit dataset (i.e. `sf`) that contains information on the site locations and additional information for each site too. Here, each row corresponds to a different site and each column contains different information about the sites. The `f1` column contains the results from previous surveys, where ones indicate that the feature was previously detected at a site, zeros indicate that the feature has not previously been detected at a site, and missing (`NA`) values mean that a site has not yet been surveyed. The `p1` column contains modelled probability of occupancy predictions from environmental niche models. The `management_cost` column contains the cost for managing each site for conservation. Although the `site_data` object contains additional columns, they are not relevant here and so we will not bother with them. To help understand the simulated data, let's create some visualizations.

```{r, fig.height = h, fig.width = w * 2.0}
# map of site occupancy data from previous surveys
p1 <-
  site_data %>%
  mutate(f1 = as.character(f1)) %>%
  ggplot() +
  geom_sf(aes(color = f1)) +
  scale_color_manual(name = "", values = c("1" = "red", "0" = "black"),
                     na.value = "blue") +
  labs(title = "presence/absence data") +
  theme(legend.position = "bottom")

# make map of modelled probability of occupancy data
p2 <-
  site_data %>%
  ggplot() +
  geom_sf(aes(color = p1)) +
  scale_color_viridis(name = "", limits = c(0, 1)) +
  labs(title = "probability data") +
  theme(legend.position = "bottom")

# make map of site occupancy data from previous surveys
p3 <-
  site_data %>%
  ggplot() +
  geom_sf(aes(color = management_cost)) +
  scale_color_viridis(name = "") +
  labs(title = "management costs") +
  theme(legend.position = "bottom")

# display plots
grid.arrange(p1, p2, p3, nrow = 1)
```

Next we will simulate a total budget for protecting sites. Specifically, we will set this budget (i.e. `total_budget`) as 60% of the overall site management costs.

```{r}
# calculate total budget for protecting sites
total_budget <- sum(site_data$management_cost) * 0.6

# print budget
print(total_budget)
```

After simulating data for the sites, we will simulate data for the conservation feature.

```{r}
# simulate feature data
feature_data <- simulate_feature_data(
  n_features = 1, proportion_of_survey_features = 1)

# print feature data
print(feature_data)
```

The `feature_data` object is a table (i.e. `tibble`) that contains information on the conservation feature. Here, each row corresponds to a different feature -- and so it only has one row because we only have one feature -- and each column contains different information about the feature(s). The `name` column contains the name of the feature. The `alpha` and `gamma` columns contain values that specify the conservation benefit gained when sites are managed for conservation. The `survey_sensitivity` and `survey_specificity` columns denote the sensitivity (probability of correctly detecting a presence) and specificity (probability of correctly detecting an absence) of the survey methodology. Finally, the `model_sensitivity` column denotes the sensitivity of the model used to predict the occupancy probabilities in the `site_data` object (i.e. values in the `p1` column).

## Benchmark analysis

After simulating the data, we can conduct our benchmark analysis. Our simulated data contains one feature and ten planning units in total. This means that there is a total of `r n_states(1, 10)` states. Since we are interested in understanding how the number states in the approximation method affects its accuracy, we will create a set of containing different numbers of states (i.e. `n_approx_states`).

```{r}
# create set of number of approximation states
n_approx_states <- ceiling(seq(10, n_states(1, 10), length.out = 10))

# print states
print(n_approx_states)
```

Next, we will benchmark the approximation methods for calculating the expected value of the decision given current information using the simulated data and these varying numbers of approximation states. To account for stochasticity in the approximation methods -- i.e. each run will give a different answer depending on the state of the random number generator -- we will report means and standard errors for 25 replicates per number of approximation states.

```{r}
# calculate the correct value of the decision given current information using
# exact methods
ev_current <- expected_value_of_decision_given_current_information(
  site_data = site_data,
  feature_data = feature_data,
  site_occupancy_columns = "f1",
  site_probability_columns = "p1",
  site_management_cost_column = "management_cost",
  feature_survey_sensitivity_column = "survey_sensitivity",
  feature_survey_specificity_column = "survey_specificity",
  feature_model_sensitivity_column = "model_sensitivity",
  feature_alpha_column = "alpha",
  feature_gamma_column = "gamma",
  total_budget = total_budget)

# estimate the correct value of the decision given current information using
# approximation methods
ev_prime_current <-
  plyr::ldply(n_approx_states, function(x) {
    result <-
      approx_expected_value_of_decision_given_current_information(
        site_data = site_data,
        feature_data = feature_data,
        site_occupancy_columns = "f1",
        site_probability_columns = "p1",
        site_management_cost_column = "management_cost",
        feature_survey_sensitivity_column = "survey_sensitivity",
        feature_survey_specificity_column = "survey_specificity",
        feature_model_sensitivity_column = "model_sensitivity",
        feature_alpha_column = "alpha",
        feature_gamma_column = "gamma",
        total_budget = total_budget,
        n_approx_replicates = 25,
        n_approx_states_per_replicate = x,
        seed = 100)
    data.frame(n = x, mean = result[[1]], se = result[[2]])
  }) %>%
  as_tibble()

# preview results
print(ev_current)
print(ev_prime_current)
```

In a similar manner, we will benchmark the approximation methods for calculating the expected value of the decision given perfect information.

```{r}
# calculate the correct value of the decision given perfect information using
# exact methods
ev_certainty <- expected_value_of_decision_given_perfect_information(
  site_data = site_data,
  feature_data = feature_data,
  site_occupancy_columns = "f1",
  site_probability_columns = "p1",
  site_management_cost_column = "management_cost",
  feature_survey_sensitivity_column = "survey_sensitivity",
  feature_survey_specificity_column = "survey_specificity",
  feature_model_sensitivity_column = "model_sensitivity",
  feature_alpha_column = "alpha",
  feature_gamma_column = "gamma",
  total_budget = total_budget)

# estimate the correct value of the decision given perfect information using
# approximation methods
ev_prime_certainty <-
  plyr::ldply(n_approx_states, function(x) {
    result <-
      approx_expected_value_of_decision_given_perfect_information(
        site_data = site_data,
        feature_data = feature_data,
        site_occupancy_columns = "f1",
        site_probability_columns = "p1",
        site_management_cost_column = "management_cost",
        feature_survey_sensitivity_column = "survey_sensitivity",
        feature_survey_specificity_column = "survey_specificity",
        feature_model_sensitivity_column = "model_sensitivity",
        feature_alpha_column = "alpha",
        feature_gamma_column = "gamma",
        total_budget = total_budget,
        n_approx_replicates = 25,
        n_approx_states_per_replicate = x,
        seed = 100)
    data.frame(n = x, mean = result[[1]], se = result[[2]])
  }) %>%
  as_tibble()

# preview results
print(ev_certainty)
print(ev_prime_certainty)
```

Now let's create a plot to visualize the results. This plot has two panels. The top panel corresponds to the _expected value of the decision given current information_ and the bottom panel corresponds to the _expected value of the decision given perfect information_. Within each panel, points correspond to mean estimates from the approximate method, error bars show standard errors associated with estimates from the approximation method, and the blue line shows the correct value calculated using the exact methods.

```{r, fig.height = h * 2, fig.width = w * 1.5}
# prepare data for plotting
point_data <-
  ev_prime_current %>%
  mutate(metric = "expected value of decision given current information") %>%
  bind_rows(ev_prime_certainty %>%
            mutate(metric = paste("expected value of decision given",
                                  "perfect information"))) %>%
  mutate(lower = mean - se, upper = mean + se)
line_data <- tibble(
  metric = c("expected value of decision given current information",
             "expected value of decision given perfect information"),
  yint = c(ev_current, ev_certainty))

# create plot
ggplot() +
geom_point(aes(x = n, y = mean), point_data) +
geom_errorbar(aes(x = n, ymin = lower, ymax = upper), point_data) +
geom_hline(aes(yintercept = yint), line_data, colour = "#3366FF") +
facet_wrap(~ metric, ncol = 1) +
xlab("Number of states used in approximation calculations") +
ylab("Expected value") +
theme(strip.background = element_rect(color = "black", fill = "black"),
      strip.text = element_text(color = "white", size = 12))
```

We can see that the estimates (points) get closer to the correct value (blue line) when more states are used in the approximation calculations (greater values on the x-axis). Furthermore, we can see that the estimates become more precise (smaller error bars) when more states are used in the approximation calculations (greater values on the x-axis). These results demonstrate that the approximation method is more accurate when we use a greater number of states for the calculations. Indeed, the approximation method yields roughly equivalent values to the exact method when using a large number of states for the calculations.

## References
